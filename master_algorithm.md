# [《终极算法》](https://item.jd.com/12079958.html?dist=jd) 书摘（20170709）

内容简介：   
在《终极算法》中，全球著名的算法问题专家、机器学习领域的先驱人物佩德罗·多明戈斯，为我们揭开了算法的神秘面纱，让我们一窥谷歌以及你的智能手机背后的机器学习原理。他阐释了机器学习的五大学派思想，解释了它们如何将神经科学、心理学、物理等领域的理论转变为算法并为你服务，并提出了“终极算法”的设想，探讨了终极算法对未来商业、科学、社会以及对每个人的意义。对于想要理解未来将发生怎样的变革、以及想走在变革前沿的人来说，这是一本必不可少的思想指南。       
“如果这种终极算法存在，那么它将可以通过数据获得世界上过去、现在、未来的所有知识。这个算法的发明将会是科学史上伟大的进步之一。”   

     
**对终极算法的探讨，跟对大一统理论的探讨一样激动人心。探讨过程环环相扣，所以也不全然是书摘了，还有一些碎碎念，2333**    


## 符号学派
- 哲学家最热衷讨论的问题
1. 理性主义    
柏拉图、笛卡尔、斯宾诺莎、莱布尼茨    
2. 经验主义    
亚里士多德、洛克、贝克莱、休谟   

归纳中的假设vs假设风险（[“归纳主义者火鸡”](https://www.douban.com/group/topic/4929875/)，未来和昨天一样vs实际可能不一样）        
而如果没有这样的假设，知识和生活将不复存在   

休谟－符号学派的守护神，提出：在概括我们见过的东西以及没见过的东西时，怎样才能做到合理？从某种意义上，每种学习算法都在尝试回答这个问题。     
如何将某种结果推广到我们没见过的事件中。    

draming：观察现象－>分析总结规律并验证－>分析更深层次的规律或机制（很可能是很简单的，不太能想象“神”会笨到单独创建每样东西每件事情，最根本的规律应该是统一并且极其简单又能引发变化的）－>领会“神”的意图。前三步应该是能够做到的，至于最后能否领会到“神”的意图可能会受限于人类所处的维度或者理解力吧。“神”指万物规律。

- [“没有免费午餐定理”](https://baike.baidu.com/item/%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86/8848514?fr=aladdin)——David Wolpert    
怎样才算是好的学习算法？没有哪个算法比得上随即猜测。但我们不关心所有可能存在的世界，而只关心我们生存的这个世界，把对这个世界的了解输入学习算法，那么和随即猜测相比，算法就可以发挥优势了。    
机器学习必须有所假设，从零开始只会一无所获。    
机器学习不能完全从零开始，必须把一些我们已知的真实形式化到系统中。首先做有条件的假设，如果这样无法解释数据，再放松假设的条件，这就是典型的机器学习。

- 过拟合    
好的学习算法永远在已知和幻觉的夹缝中行走。    
幻觉－过拟合，每个强大的学习算法都会遇到的问题，人类也会出现过拟合的情况，尤其是小孩子的时候（也许是因为输入的数据还少吧）。数据中的噪声被严重夸大，误差／偶然事件也被当作规则。    

- 合取vs规则集    
规则集在很大程度上比合取概念要有力很多，但规则集太过细致的话则会无法用于判断。    
学习意味着将细节遗忘，只记住重要部分。（博尔赫斯《博闻强识的富内斯》。draming：所以说要赋予AI抽象能力）    

“我们见过的所有真实的东西，在宇宙中也是真实的”——牛顿    

机器学习不是时时刻刻都知道自己是否可以准确无误的运行。（draming：23333，其实人也一样啊，要是能把人的日志和评价函数结果调出来，想必bug和错误无数）    

- 归纳是逆向的演绎    
在归纳推理中会以最初事实和衍生事实作为开始，然后找出一个规则，让我们用先者推出后者。    
我们以越多的规则和事实作为开头，也就有越多的机会运用“逆向演绎”归纳新的规则。我们归纳的规则越多，我们能归纳的规则也就越多，这是知识创造的良性循环，只受过拟合和计算成本的限制。    

- 符号学派的核心理念    
所有和智力相关的工作都可以归结为对符号的操纵。根据这个假设，智力是独立于基质的，软件可以和硬件清晰的分离。（draming：那是把灵魂复制走好呢还是剪切走好呢，也许灵魂有防复制功能吧。。。）    

- 心理学家，大卫马尔认为，每个信息处理系统应该经过三个不同水平的研究：    
1. 该系统解决所解决问题的基本属性    
2. 用来解决问题的算法和表示方法    
3. 这些算法和表示方法如何实现    

- 符号主义机器学习是人工智能知识工程学派的一个分支     
20世纪70年代，所谓基于知识的系统取得卓越成绩，到了80年代，迅速传播，后来消失。主要原因：从专家身上提取知识，然后将其编码成规则，难度太大，易出故障。
因为其起源和指导原则，与其他学派相比，和人工智能其他方面关系更为密切。    

- 决策树vs逆向演绎    
规则集和决策树相比，表达多数概念的方式要简洁得多    
决策树转规则集很容易，反之则很难     
就像一个超级科学家，查看论据，思考可行的归纳法，整理最有利的证据，然后将这些和其他论据一起，进一步提出假设。    

- 逆向演绎的缺点    
可行的归纳法数量广泛，除非和最初知识保持密切关系，否则很容易在空间中迷失    
容易被噪声迷惑    
真正的概念很少能通过一个规则集来定义，还没有人能只学习一个规则组，就能通过观看图片的像素认出一只猫，可能以后也不能。    

- 联结学派对符号学派尤其不满    
通过逻辑规则定义的概念仅仅是冰山一角，表面之下有很多东西是形式推理无法看到的。    



## 联结学派

- [唐纳德赫布](https://baike.baidu.com/item/%E5%94%90%E7%BA%B3%E5%BE%B7%C2%B7%E8%B5%AB%E5%B8%83/6487221?fr=aladdin)     
[赫布律](https://baike.baidu.com/item/%E8%B5%AB%E5%B8%83%E5%BE%8B/2960219?fr=aladdin) 联结主义的奠基石，相信知识储存在神经元之间的联结关系中。
“一起放电的神经元也会被串连在一起。”

- 符号学派vs联结学派
1. 符号学派
符号和它们代表的概念之间有一一对应的关系。
按次序的。在逆向演绎中，我们可以一步一步的弄明白，为了从前提出发得到满意的结论，需要哪些新规则。
2. 联结学派
代表方式是分散的，每个概念由许多神经元来表示，而每个神经元又会和其他神经元一起代表许多不同的概念，互相激发的神经元会形成赫布所称的“细胞集”。
按照赫布律，所有神经元同时进行学习。
draming：底层结构应该是联结学派，向上抽象应该是符号学派和其他吧。

- 计算机vs人脑
1. 计算机
很多步骤，但每步很快
耗能大，沃森消耗的电能点亮整栋办公楼
2. 人脑
可以同时进行多项运算，数十亿神经元同时起作用，但每项很慢，神经元最多每秒放电1000次
人脑消耗的能量仅相当于一个小灯泡
大脑里轴突的长度相当于地球到月球的距离（draming：23333）



